# -*- coding: utf-8 -*-
"""Data preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GJd3BcR_FnZ14OSkxi1sYxc_RZdxlCGk

# 1) Import packages and data
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt # data visualization
import seaborn as sns # statistical data visualization
# %matplotlib inline

import warnings

warnings.filterwarnings('ignore')

# get data from drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/Colab Notebooks/

df = pd.read_csv('Cocoon_official_15042023.csv')

df.info()

"""# 2) Simple processing on null and unnecessary data

1) Eliminate 'URL' column as I will not be needing it in the future
"""

df=df.drop(columns=['URL'])

"""2) Filter to eliminate null value in "Name". For sales data, later on, it will use filter code which is only aplicaple with nonnull data (step 4)"""

df = df.dropna(subset=['Name', 'Sales'])

df = df.dropna(subset=['Name'])

print(df)

# check if df reach needed non-null features and check for type of data for the next step
df.info()
df.isnull().sum()

"""# 3) Date"""

from datetime import datetime
# filter dataframe into two subset right_dates and error_dates, which are date in the right format "mm/dd/yyyy" and those aren't 
e_dates = pd.to_datetime(df['Date'], format='%m/%d/%Y', errors='coerce').isna()
error_dates = df[e_dates]

r_dates = pd.to_datetime(df['Date'], format='%m/%d/%Y', errors='coerce').notna()
right_dates = df[r_dates]

#change date in wrong format to right format
error_dates['Date'] = pd.to_datetime(error_dates['Date'], format='%d/%m/%Y').dt.strftime('%m/%d/%Y')

#combine subset back to one original one
df = pd.concat([error_dates, right_dates], ignore_index=True)

"""# 4) Sales

4.1) Remove "Da ban" part in "Sales" and  to make "Sales" in the same structure.
"""

da_ban_data = df[df['Sales'].str.startswith('Đã bán')]
non_da_ban_data = df[~df['Sales'].str.startswith('Đã bán')]
# remove "Đã bán" part from beginning of each string
da_ban_data['Sales'] = da_ban_data['Sales'].str.replace('Đã bán', '')
# merge filtered dataset back into original dataset
df = pd.concat([da_ban_data, non_da_ban_data], ignore_index=True)

"""4.2) turn "k" to thousands and change data type (to make "Sales" in the same type and structure)"""

def convert_to_numeric(strings):
    if "k" in strings:
        num = float(strings.replace("k", "").replace(',', '.'))*1000
        return num
    else:
        return strings
df['Sales'] = df['Sales'].apply(convert_to_numeric)
# there are data that is empty string as initially it can be "Da ban" with no sale number
# therefore, replace those with a valid value or remove the row entirely and change column type to numeric
df['Sales'] = pd.to_numeric(df['Sales'], errors='coerce').fillna(0)

df.info()

# print the names of the products with sales equal to 0
zero_sales = df[df["Sales"] == 0.0]
zero_sales.info()

"""# 5) Rating and Rating record

"""

df.info()

df['Rating record'] = df['Rating record'].fillna(value='0')

def convert_to_numeric(strings):
    if "k" in strings:
        num = float(strings.replace("k", "").replace(',', '.'))*1000
        return num
    else:
        return strings

df['Rating record'] = df['Rating record'].apply(convert_to_numeric)
# there are data that is empty string as initially it can be "Da ban" with no sale number
# therefore, replace those with a valid value or remove the row entirely and change column type to numeric
df['Rating record'] = pd.to_numeric(df['Rating record'], errors='coerce').fillna(0)

df

df['Rating'] = df['Rating'].str.replace('Chưa Có Đánh Giá', '0')

# therefore, replace those with a valid value or remove the row entirely and change column type to numeric
df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce').fillna(0)
df = df.dropna(subset=['Rating'])

"""# 6) Price"""

# extract only the numeric part of the "Price" column
df["Price"] = df["Price"].str.extract(r"(\d+\.?\d*)").astype(float)

# extract only the numeric part of the "Discount" column
df["Discount"] = df["Discount"].str.extract(r"(\d+\.?\d*)").astype(float)
df.columns = df.columns.str.replace('Discount', 'Discount(%)')

"""# 7) Category and Product ID"""

# define a function to assign categories based on product names
def assign_category(name):
    if "cà phê" in name.lower():
        return "Coffee"
    elif "hoa hồng" in name.lower():
        return "Rose"
    elif "bưởi" in name.lower():
        return "Pomelo"
    elif "nghệ" in name.lower():
        return "Ginger"
    elif "bí đao" in name.lower():
        return "Squash"
    else:
        return "Other"

# define a function to assign categories based on product names
def assign_ini(name):
    if "cà phê" in name.lower():
        return "C"
    elif "hoa hồng" in name.lower():
        return "R"
    elif "bưởi" in name.lower():
        return "P"
    elif "nghệ" in name.lower():
        return "G"
    elif "bí đao" in name.lower():
        return "S"
    else:
        return "O"
# apply the function to create a new "category" column
df["Category"] = df["Name"].apply(lambda name: assign_category(name))
df["Ini_cat"] = df["Name"].apply(lambda name: assign_ini(name))

#create seperate dataset for each category
coffee_df = df[df["Category"] == "Coffee"]
rose_df = df[df["Category"] == "Rose"]
pomelo_df = df[df["Category"] == "Pomelo"]
ginger_df = df[df["Category"] == "Ginger"]
squash_df = df[df["Category"] == "Squash"]

# group by product name and assign a product ID to each category
coffee_df["Group_ID"] = coffee_df.groupby("Name").ngroup() + 1
rose_df["Group_ID"] = rose_df.groupby("Name").ngroup() + 1
pomelo_df["Group_ID"] = pomelo_df.groupby("Name").ngroup() + 1
ginger_df["Group_ID"] = ginger_df.groupby("Name").ngroup() + 1
squash_df["Group_ID"] = squash_df.groupby("Name").ngroup() + 1

# merge data with category type and ID back as official dataset 
df=pd.concat([coffee_df, rose_df, pomelo_df, ginger_df, squash_df], ignore_index=True)

# create Product_ID column based on category initial and category ID
df['Product_ID'] = df.apply(lambda row: str(row['Ini_cat']) + str(row['Group_ID']), axis=1)

discount_program = df[df["Discount(%)"].notna()]
discount_program

# Final check for result of preprocessing data: 
df.info()

# Record information of the final data set as "final_data_info" 
final_data_info= df.info()

# Write the DataFrame to a CSV file
df.to_csv("Preprocessed_data.csv", index=True)
coffee_df.to_csv("Coffee_data.csv", index=True)
rose_df.to_csv("Rose_df.csv", index=True)
pomelo_df.to_csv("Pomelo_df.csv", index=True)
ginger_df.to_csv("Ginger_df.csv", index=True)
squash_df.to_csv("Squash_df.csv", index=True)

df

df['Rating'].value_counts()

"""# 8) Check data information for final result

"""

df1= pd.read_csv('Preprocessed_data.csv')

#print(df.describe())
df.describe()

df1.info()

print(df1.columns)

df1=df1.drop(columns=['Unnamed: 0','Group_ID'])

# specify a formatting function that converts the scientific notation to a fixed-point notation
pd.options.display.float_format = '{:.2f}'.format
# Rename the columns to include count, std, and mean
stats = df1.describe()
#save result of statiscal dataset information into csv file 
stats.to_csv("Statiscal result.csv", index= True)
stats